{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_length,\n",
    "        train_path,\n",
    "        mode='word',\n",
    "        train=True,\n",
    "        train_split=0.8\n",
    "    ):\n",
    "        self.seq_length = seq_length\n",
    "        self.mode = mode\n",
    "        self.train_split = train_split\n",
    "        self.train_path = train_path\n",
    "        self.all_data, self.train_data, self.eval_data = self._read_data()\n",
    "        \n",
    "        self.unique_data = self._find_unique()\n",
    "\n",
    "        self.idx_data = {idx: data for idx, data in enumerate(self.unique_data)}\n",
    "        self.data_idx = {data: idx for idx, data in enumerate(self.unique_data)}\n",
    "        \n",
    "        self.data = self.train_data if train else self.eval_data\n",
    "\n",
    "        self.indexed_data = np.array([self.data_idx[i] for i in self.data])\n",
    "        \n",
    "        self.indexed_data = torch.from_numpy(self.indexed_data)\n",
    "\n",
    "    def _read_data(self):\n",
    "        text = open(self.train_path, 'rb').read().decode(encoding='utf-8')\n",
    "        data = pd.Series(list(re.sub(\"[\" + '\\r\\ufeff' + \"]\", '', text))) if self.mode =='char' \\\n",
    "                    else pd.Series(re.findall(r\"[\\w']+|[.,!?;]\", text))\n",
    "        return data, data[:int(len(data) * self.train_split)], \\\n",
    "                            data[int(len(data) * (1 - self.train_split)):]\n",
    "\n",
    "    def _find_unique(self):\n",
    "        data_count = Counter(self.all_data)\n",
    "        return sorted(data_count, key=data_count.get, reverse=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.indexed_data) - self.seq_length)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.indexed_data[idx:idx+self.seq_length],\n",
    "            self.indexed_data[idx+1:idx+self.seq_length+1],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(20, 'language_modelling/data/penn/train.txt', \n",
    "                  'language_modelling/data/penn/valid.txt', mode='word', train=True)\n",
    "trainloader = DataLoader(train_dataset, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = Dataset(20, 'language_modelling/data/penn/train.txt', \n",
    "                  'language_modelling/data/penn/valid.txt', mode='word', train=False)\n",
    "evalloader = DataLoader(eval_dataset, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(20, 'language_modelling/data/alice_wonderland.txt', mode='word', train=True)\n",
    "trainloader = DataLoader(train_dataset, batch_size=20)\n",
    "eval_dataset = Dataset(20, 'language_modelling/data/alice_wonderland.txt', mode='word', train=False)\n",
    "evalloader = DataLoader(eval_dataset, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, embed_size=128, hidden_size=128,\n",
    "                 hidden_layers=3, seq_length=20, dropout=0.2):\n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.ntoken = ntoken\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_length = seq_length\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embed = nn.Embedding(\n",
    "            num_embeddings=self.ntoken,\n",
    "            embedding_dim=self.embed_size,\n",
    "        )\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=self.embed_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=self.hidden_layers,\n",
    "            dropout=self.dropout\n",
    "        )\n",
    "        self.linear = nn.Linear(self.hidden_size, ntoken)\n",
    "\n",
    "    def forward(self, x, state_h):\n",
    "        embed = self.embed(x)\n",
    "        output, state = self.rnn(embed, state_h)\n",
    "        logits = self.linear(output)\n",
    "        return logits, (state)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def initialize(self, seq_length=None):\n",
    "        if seq_length is None:\n",
    "            seq_length = self.seq_length\n",
    "        return torch.zeros(self.hidden_layers, seq_length, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, embed_size=128, hidden_size=128,\n",
    "                 hidden_layers=3, seq_length=20, dropout=0.2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.ntoken = ntoken\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_length = seq_length\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embed = nn.Embedding(\n",
    "            num_embeddings=self.ntoken,\n",
    "            embedding_dim=self.embed_size,\n",
    "        )\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=self.embed_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=self.hidden_layers,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.linear = nn.Linear(self.hidden_size, ntoken)\n",
    "\n",
    "    def forward(self, x, state):\n",
    "        embed = self.embed(x)\n",
    "        output, state = self.rnn(embed, state)\n",
    "        logits = self.linear(output)\n",
    "        return logits, state\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def initialize(self, seq_length=None):\n",
    "        if seq_length is None:\n",
    "            seq_length = self.seq_length\n",
    "        return (torch.zeros(self.hidden_layers, seq_length, self.hidden_size),\n",
    "                torch.zeros(self.hidden_layers, seq_length, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, criterion, optimizer, dataloader,\n",
    "                    log=True, log_interval=200):\n",
    "    model.train()\n",
    "    state = model.initialize()\n",
    "    total_loss = 0\n",
    "    for batch, (x, y) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred, state = model(x, state)\n",
    "        loss = criterion(y_pred.transpose(-2, -1), y)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        \n",
    "        state = tuple([s.detach() for s in state]) if type(state) is tuple else state.detach()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % log_interval == 0 and log:\n",
    "            print({'Batch {} / {}, loss: {}'.format(batch, len(dataloader),\n",
    "                                                    total_loss / (batch + 1)) })\n",
    "    return total_loss / (len(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, criterion, dataloader, log=True):\n",
    "    model.eval()\n",
    "    state = model.initialize()\n",
    "    total_loss = 0\n",
    "    for batch, (x, y) in enumerate(dataloader):\n",
    "        with torch.no_grad():\n",
    "            y_pred, state = model(x, state)\n",
    "        loss = criterion(y_pred.transpose(-2, -1), y)\n",
    "        total_loss += loss.item()\n",
    "    if log:\n",
    "        print({'Evaluation loss': total_loss / (len(dataloader)) })\n",
    "    return total_loss / (len(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainloader, evalloader, model, optimizer, \n",
    "          criterion, nepoch, log_interval=1, eval_during_train=True, \n",
    "          eval_interval=1, save_interval=1, model_name='model'):\n",
    "    for ep in range(nepoch):\n",
    "        train_loss = train_one_epoch(model, criterion, \n",
    "                                     optimizer, trainloader)\n",
    "        if ep % log_interval == 0:\n",
    "            print({'Epoch': ep, 'loss': train_loss})\n",
    "        if eval_during_train and ep % eval_interval == 0:\n",
    "            eval_loss = evaluate(model, criterion, evalloader)\n",
    "        if ep % save_interval == 0:\n",
    "            torch.save(model.state_dict(), 'language_modelling/models/'\n",
    "                       + model_name + '.p')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, data, data_idx_dict, \n",
    "             idx_data_dict, len_hist=50, len_gen=50):\n",
    "    model.eval()\n",
    "    state = model.initialize(len_hist)\n",
    "    for i in range(len_gen):\n",
    "        x = torch.tensor([[data_idx_dict[d] for d in data[-len_hist-1:-1]]])\n",
    "        with torch.no_grad():\n",
    "            y_pred, state = model(x, state)\n",
    "        last_logits = y_pred[0][-1]\n",
    "        prob = nn.functional.softmax(last_logits, dim=0).detach().numpy()\n",
    "        idx = np.random.choice(len(last_logits), p=prob)\n",
    "        data.append(idx_data_dict[idx])\n",
    "    return ' '.join(data[-(len_hist+len_gen):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Batch 0 / 1415, loss: 8.175182342529297'}\n",
      "{'Batch 200 / 1415, loss: 6.495981456035405'}\n",
      "{'Batch 400 / 1415, loss: 6.292173188226181'}\n",
      "{'Batch 600 / 1415, loss: 6.191937619556802'}\n",
      "{'Batch 800 / 1415, loss: 6.1231154722816195'}\n",
      "{'Batch 1000 / 1415, loss: 6.065866774731464'}\n",
      "{'Batch 1200 / 1415, loss: 5.9941749658116095'}\n",
      "{'Batch 1400 / 1415, loss: 5.9446944896022735'}\n",
      "{'Epoch': 0, 'loss': 5.940430346478843}\n",
      "{'Evaluation loss': 5.722703741969995}\n",
      "{'Batch 0 / 1415, loss: 6.667312145233154'}\n",
      "{'Batch 200 / 1415, loss: 5.525613713620314'}\n",
      "{'Batch 400 / 1415, loss: 5.390993892403315'}\n",
      "{'Batch 600 / 1415, loss: 5.331671307765307'}\n",
      "{'Batch 800 / 1415, loss: 5.271353118875053'}\n",
      "{'Batch 1000 / 1415, loss: 5.2132663974514255'}\n",
      "{'Batch 1200 / 1415, loss: 5.163024153141654'}\n",
      "{'Batch 1400 / 1415, loss: 5.126555526367176'}\n",
      "{'Epoch': 1, 'loss': 5.123882277020296}\n",
      "{'Evaluation loss': 5.352085865681247}\n",
      "{'Batch 0 / 1415, loss: 6.13288688659668'}\n",
      "{'Batch 200 / 1415, loss: 5.057281788308822'}\n",
      "{'Batch 400 / 1415, loss: 4.91331696985963'}\n",
      "{'Batch 600 / 1415, loss: 4.848499942738284'}\n",
      "{'Batch 800 / 1415, loss: 4.787379266915101'}\n",
      "{'Batch 1000 / 1415, loss: 4.738386747720358'}\n",
      "{'Batch 1200 / 1415, loss: 4.696144871866574'}\n",
      "{'Batch 1400 / 1415, loss: 4.656506819694405'}\n",
      "{'Epoch': 2, 'loss': 4.653904650489348}\n",
      "{'Evaluation loss': 5.204542022374831}\n",
      "{'Batch 0 / 1415, loss: 5.685987949371338'}\n",
      "{'Batch 200 / 1415, loss: 4.686055506046731'}\n",
      "{'Batch 400 / 1415, loss: 4.544416540578713'}\n",
      "{'Batch 600 / 1415, loss: 4.473466081349504'}\n",
      "{'Batch 800 / 1415, loss: 4.410489813665325'}\n",
      "{'Batch 1000 / 1415, loss: 4.361185216284418'}\n",
      "{'Batch 1200 / 1415, loss: 4.331710047765537'}\n",
      "{'Batch 1400 / 1415, loss: 4.290383021206961'}\n",
      "{'Epoch': 3, 'loss': 4.287546349919727}\n",
      "{'Evaluation loss': 5.0855611964586345}\n",
      "{'Batch 0 / 1415, loss: 5.2220563888549805'}\n",
      "{'Batch 200 / 1415, loss: 4.350011507670085'}\n",
      "{'Batch 400 / 1415, loss: 4.211084121480547'}\n",
      "{'Batch 600 / 1415, loss: 4.14157184229516'}\n",
      "{'Batch 800 / 1415, loss: 4.084715600317337'}\n",
      "{'Batch 1000 / 1415, loss: 4.041928365871265'}\n",
      "{'Batch 1200 / 1415, loss: 4.009780814506728'}\n",
      "{'Batch 1400 / 1415, loss: 3.973183785992635'}\n",
      "{'Epoch': 4, 'loss': 3.970321260408462}\n",
      "{'Evaluation loss': 5.083781164526518}\n",
      "{'Batch 0 / 1415, loss: 4.665773868560791'}\n",
      "{'Batch 200 / 1415, loss: 4.076386506284647'}\n",
      "{'Batch 400 / 1415, loss: 3.956194118965891'}\n",
      "{'Batch 600 / 1415, loss: 3.8908720619468244'}\n",
      "{'Batch 800 / 1415, loss: 3.8302804289685652'}\n",
      "{'Batch 1000 / 1415, loss: 3.791010930702522'}\n",
      "{'Batch 1200 / 1415, loss: 3.756357777227867'}\n",
      "{'Batch 1400 / 1415, loss: 3.7191370493339524'}\n",
      "{'Epoch': 5, 'loss': 3.7167651830208177}\n",
      "{'Evaluation loss': 4.991272486225034}\n",
      "{'Batch 0 / 1415, loss: 4.5457940101623535'}\n",
      "{'Batch 200 / 1415, loss: 3.795407672426594'}\n",
      "{'Batch 400 / 1415, loss: 3.6968243145883233'}\n",
      "{'Batch 600 / 1415, loss: 3.629972575706571'}\n",
      "{'Batch 800 / 1415, loss: 3.5772403146741394'}\n",
      "{'Batch 1000 / 1415, loss: 3.5426760217645668'}\n",
      "{'Batch 1200 / 1415, loss: 3.52029920994094'}\n",
      "{'Batch 1400 / 1415, loss: 3.4847642265839887'}\n",
      "{'Epoch': 6, 'loss': 3.482661863266369}\n",
      "{'Evaluation loss': 4.990243166664043}\n",
      "{'Batch 0 / 1415, loss: 4.153111934661865'}\n",
      "{'Batch 200 / 1415, loss: 3.5689958588993966'}\n",
      "{'Batch 400 / 1415, loss: 3.4557664792733895'}\n",
      "{'Batch 600 / 1415, loss: 3.4012062680503097'}\n",
      "{'Batch 800 / 1415, loss: 3.354527834798215'}\n",
      "{'Batch 1000 / 1415, loss: 3.3209842561365486'}\n",
      "{'Batch 1200 / 1415, loss: 3.299474908350707'}\n",
      "{'Batch 1400 / 1415, loss: 3.264436306058297'}\n",
      "{'Epoch': 7, 'loss': 3.262381429470049}\n",
      "{'Evaluation loss': 4.986852156484085}\n",
      "{'Batch 0 / 1415, loss: 3.7375032901763916'}\n",
      "{'Batch 200 / 1415, loss: 3.347368052942836'}\n",
      "{'Batch 400 / 1415, loss: 3.250979666698009'}\n",
      "{'Batch 600 / 1415, loss: 3.1866816367563513'}\n",
      "{'Batch 800 / 1415, loss: 3.145608501636729'}\n",
      "{'Batch 1000 / 1415, loss: 3.1145079150185597'}\n",
      "{'Batch 1200 / 1415, loss: 3.092901575674522'}\n",
      "{'Batch 1400 / 1415, loss: 3.0579676774890827'}\n",
      "{'Epoch': 8, 'loss': 3.056022802469166}\n",
      "{'Evaluation loss': 4.9893193720928775}\n",
      "{'Batch 0 / 1415, loss: 3.472846269607544'}\n",
      "{'Batch 200 / 1415, loss: 3.12766161961342'}\n",
      "{'Batch 400 / 1415, loss: 3.0353681232566547'}\n",
      "{'Batch 600 / 1415, loss: 2.973746155343714'}\n",
      "{'Batch 800 / 1415, loss: 2.9404302128542974'}\n",
      "{'Batch 1000 / 1415, loss: 2.9143905214496426'}\n",
      "{'Batch 1200 / 1415, loss: 2.899204571876399'}\n",
      "{'Batch 1400 / 1415, loss: 2.867704359911579'}\n",
      "{'Epoch': 9, 'loss': 2.866214878651784}\n"
     ]
    }
   ],
   "source": [
    "nepoch = 1000\n",
    "lr = 0.001\n",
    "\n",
    "ntoken = len(train_dataset.unique_data)\n",
    "\n",
    "model = LSTMModel(ntoken)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "train(trainloader, evalloader, model, optimizer, criterion, nepoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset.indexed_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Evaluation loss': 1.5122261361696234}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.5122261361696234"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, criterion, evalloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"h t   b e a r   h i s   m e m o r y : \\n     B u t   t h o u ,   c o n t r a c t e d   t o   t h i n e   o w n   b r i g h t   e y e s , \\n     F e e d ' s t   t h y   l i g h t ' s   f l a m e   w i t f   o e f   \\n o   u v p e r n \\n e     A   n d a o y k , g   e l i t d h   e o a r t s h     v t e a n l s e \\n d     m   e a n r t u ' a n d d   ' e g t r i e a   t v h e e r , n \\n       S   o n f   u n n h e i r l s y     n n e o i r d ,     y c o e u d t   h w e a n r t e   d d i i n n   e d n d o \\n ,   - R F u e r v   e e n v d e e r t , -   - a t n   e f v v e o r t     I n   f I r l ' e d \\n .\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model, list(train_dataset.data.values[:300]), train_dataset.data_idx, train_dataset.idx_data, len_hist=100, len_gen=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
